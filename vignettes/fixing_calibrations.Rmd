---
title: "fixing_calibrations"
output: html_document
---

# Introduction

This vignette demonstrates the complete workflow for back-calibrating water 
quality sensor data using calibration information extracted from HTML calibration 
files. The process corrects sensor drift and applies temporal interpolation 
between calibrations to produce accurate time series data.

# Prepare Environment
```{r setup}
knitr::opts_chunk$set(
  eval = FALSE,
  echo = TRUE
)
```

```{r}
#Installing and loading all packages
invisible(
  lapply(c(
    "tidyverse", # Data manipulation
    "janitor", # Clean dirty data
    "lubridate", # Date-Time Manipulation
    "rvest", # HTML Retrieval and Manipulation
    "readxl", # Reading excel files
    "here", # Easy, cross platform file referencing
    "ggplot2", # Plotting libraries
    "ggpubr",
    "plotly",
    "devtools", # For downloading GitHub packages
    "remotes",
    "yaml",
    "arrow" # reading parquet files
  ),
  function(x) {
    if (x %in% installed.packages()) {
      suppressMessages({
        library(x, character.only = TRUE)
      })
    } else {
      suppressMessages({
        install.packages(x)
        library(x, character.only = TRUE)
      })
    }
  })
)

load_all()
```

The environment setup loads essential packages for data manipulation and 
sources all calibration functions needed for the workflow.

# Load Calibration Data

This pulls the data with the calibration provenance in it.

```{r load-cal-data}
load_calibration_data(
  cal_data_file_path = here::here("data", "collated", "sensor", "cal_reports", "munged_calibration_data.RDS"),
  update = TRUE,
  field_cal_dir = here::here("data", "raw", "sensor", "calibration_reports"),
  benchtop_cal_dir = here::here("data", "raw","sensor", "calibration_reports", "benchtop_calibrations")
)

# TODO: incorporate pH back calibrations
# TODO: still need to find the right measure to determine if a calibration is bad 

# Temporarily remove pH to test this
calibration_data <- calibration_data %>%
  map(function(year_list){
    year_list[!str_detect(names(year_list), "pH")]
  })
```

The `load_calibration_data()` function loads the calibration coefficients and 
drift correction information which were extracted from the HTML calibration files,
if they exist within the file path which is provided to it. 

If the data does not exist, or the `update` option is set to `TRUE`, this function 
calls the `cal_extract_markup_data()`, which generates this calibration data 
fresh from raw HTML files from the directory provided to it. By default these 
directories are: `field_cal_dir = here("data", "calibration_reports")` and
`benchtop_cal_dir = here("data", "calibration_reports", "benchtop_calibrations")`.
Though, note that the arguments for `cal_extract_markup_data()` can be changed 
within `load_calibration_data()` via the `...` arguments. Note that `cal_extract_markup_data()`
takes a minute to finish running.

`cal_extract_markup_data()` will establish "calibration succession" when it pulls
in the HTML data. Here "calibration succession" means that it will find the next
correct calibration that is tied to the current calibration. This is done in order
to correctly back calibrate data without using bad calibrations.

## Calibration Data Structure

The resulting `calibration_data` object follows an organizational structure 
which contains a nested list organized by year, where each year 
contains site-parameter combinations (e.g., "lincoln-pH", "tamasag-Turbidity"). 
Each site-parameter entry includes calibration coefficients (slope and offset values) 
and drift correction measurements (pre- and post-calibration standard readings),
along with their related identification and datetime information.

# Load Sensor Data

```{r load-snsr-data}
# Read in 2023, 2024, and 2025 sensor data
sensor_data <- readr::read_rds(
  here::here("data", "raw", "sensor", "manual_data_verification", 
             "complete_dataset",  "23_25_hv_pull"))
```

## Expected Sensor Data Structure

The sensor data must be organized as a year list structure where each year contains 
site-parameter lists. For example, the 2024 data would contain separate entries 
for "lincoln-pH", "lincoln-Turbidity", "tamasag-pH", etc. Each site-parameter 
combination contains time series data with datetime stamps and sensor measurements. 
This consistent structure allows the calibration functions to properly match 
sensor readings with their corresponding calibration information.

# Processing steps

## Join Sensor and Calibration Data

```{r}
sensor_calibration_data <- cal_join_sensor_calibration_data(
  # TODO: Join with field visits and make some elastic join based on the most recent site visit
  # Sometimes the calibration date does not line up with a field visit.
  sensor_data_list = sensor_data,
  calibration_data_list = calibration_data
) 
```

The `join_sensor_calibration_data()` function links each sensor measurement to 
its appropriate calibration information using temporal proximity matching. This 
function identifies the most recent calibration that applies to each sensor 
reading, creating a foundation for the back-calibration process.

## Prepare Data for Back Calibration

OLD INFO, for context of progress. The workflow has changed, but I want to keep this
here as a refresher on the previous update and for context on this update.

"""
There are some notes in the updated `cal_prepare_sensor_calibration_data` function,
but once this is finalized those will be gone. This will also get cleaned up,
but I want to explain what I did so that it can be critiqued easily. Some version 
of this will also be in the draft PR.

Okay. The previous solution had us binding consecutive "calibration chunks" together
so that we would be able to go from one calibration to the next. This update simplifies
this process a lot. Instead of doing that we make the `cal_provenance_df` object
that has a "lead" sensor calibration date and calibration coefs column that we use 
to join with the calibration chunks that we make in `cal_prepare_sensor_calibration_data`.
The code updates that follow just require pointing at a new part of the df that 
gets passed into the functions that are called in cal_back calibrate.
"""

NEW UPDATE:
Instead of using the calibration provenance data frame, now calibration succession
is established in `cal_extract_markup()` and tracked with the calibration data.
Successive, sensor specific calibrations are still used, but now with the addition
of identifying bad calibrations in order to calibrate with the correct calibrations.

```{r prep-data}
prepped_snsr_cal_data <- cal_prepare_calibration_windows(
  sensor_calibration_data_list = sensor_calibration_data
)
```

The `prepare_sensor_calibration_data()` function segments the joined data into 
calibration windows bounded by consecutive correct calibrations. Each window represents 
a time period where linear interpolation between two calibrations can be applied. 
This segmentation creates manageable chunks that allow for temporal weighting of 
calibration parameters.

Those calibration windows which are bounded by consecutive calibrations, but
which start with a bad calibration that cannot be corrected into a correct calibration
are not converted to their raw values, and they are prevented from being manipulated
in downstream back calibration functions.

# Back Calibration

```{r Apply back calibration functions to the data}
calibrated_data <- prepped_snsr_cal_data %>% 
  map(function(year){
    calibrated_site_param_list <- year %>%
      map(function(site_param){
        site_param %>% 
          map_dfr(function(chunk){
            cal_back_calibrate(chunk) 
          })
      })
  })
```

## Understanding the Nested Map Structure

The back calibration process uses an explicit nested map approach that differs 
from the simpler function calls used in previous processing steps. This design 
choice provides three levels of iteration: years, site-parameters within each 
year, and calibration chunks within each site-parameter. While `join_sensor_calibration_data()` 
and `prepare_sensor_calibration_data()` handle similar nested year/site-parameter 
iterations internally, they abstract away this complexity from the user.

The explicit nested structure in  the `back_calibration()` function call serves 
a purpose for future functionality. While automated calibration works well for 
most situations, some sensor data may require manual intervention or custom 
calibration approaches. The exposed nested maps allow users to easily extract 
specific years, site-parameters, or individual chunks for manual processing 
while maintaining the overall automated workflow for standard cases.

## The `cal_back_calibrate()` Function

The `cal_back_calibrate()` function serves as the orchestrator for the calibration 
process within each chunk. It automatically determines the appropriate calibration 
workflow based on the sensor parameter type. Standard sensors (Chlorophyll-a, 
FDOM, ORP, Pressure, Specific Conductivity, RDO) follow a three-step process 
involving temporal weighting and linear transformation back to corrected values, 
with a final validation step.

This function design ensures that users only need to call one function per chunk 
while the internal logic handles the complexity of parameter-specific calibration 
requirements.

# Saving the Back-calibrated Data

```{r save-cal-data}
write_rds(calibrated_data, here::here("data", "raw", "sensor", "manual_data_verification", "complete_dataset", "calibrated_sensor_data.rds"))
```

# Explore the calibrations

Find where calibrations could not be done and see what happened there.

```{r}
fail_list <- sapply(calibrated_data$`2024`, function(df) {
  if ("correct_calibration" %in% names(df)) {
    any(df$correct_calibration == FALSE, na.rm = TRUE)
  } else {
    FALSE
  }
})

check_list <- calibrated_data$`2024`[fail_list]
```

```{r explore-cal}
# update `calibrated_test` object with the data you would like to explore...
calibrated_test <- check_list$`sfm-Specific Conductivity`
# Extract parameter and site info for dynamic labeling
parameter <- unique(calibrated_test$parameter)
site <- unique(calibrated_test$site)
# Create a combined dataset for plotting
plot_data <- bind_rows(
  # Original data
  calibrated_test %>% 
    select(DT_round, value = mean) %>%
    mutate(correct_calibration = "Original"),
  
  # Calibrated sensor data with correct_calibration status
  calibrated_test %>%
    select(DT_round, value = mean_lm_trans, correct_calibration) %>%
    mutate(correct_calibration = as.character(correct_calibration))
)

calibration_p_test <- ggplot(plot_data) +
  geom_line(aes(x = DT_round, y = value, color = correct_calibration), 
            alpha = 0.7, size = 0.5) +
  scale_color_manual(values = c("TRUE" = "green", 
                                "FALSE" = "red", 
                                "Original" = "grey")) +
  labs(
    title = paste(site, parameter, "2024 Calibration"),
    x = "Date",
    y = paste(parameter, "(units)"),
    color = "Correct Calibration"
  ) +
  # ylim(c(0,50)) +
  theme_minimal()

# Convert to plotly
calibration_p_test <- ggplotly(calibration_p_test)
calibration_p_test
```

Some notes on this updated plot: 

Here I am highlighting when correct calibrations were used instead of bad calibrations.
Chunks of code that were calibrated using a bad calibration are in red, while
those that were calibrated with a good calibration are in green. The original
data is displayed as a point of comparison between raw and back calibrated data.

# Workflow Summary
The complete calibration workflow transforms raw sensor data through four main stages. 
First, calibration information is loaded and joined with sensor data using temporal 
matching. Second, the joined data is segmented into calibration windows for processing,
where the data is converted to its raw values based on the calibration succession
established in the original HTML extraction step. Third, each window undergoes 
back calibration using parameter-specific algorithms  that account for instrument 
characteristics and drift patterns. Finally, the calibrated  results are validated 
and compiled into a clean time series dataset ready for analysis.



